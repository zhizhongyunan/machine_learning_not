# 笔记复习

## 机器学习基础

> 任务：回归和分类

### 线性回归方程

1. 基础含义：线性关系比较强的数据适合使用$(x,\hat y)$，例如房价的预测，营业额随人数的变化。方程主要满足$y=w*x+b$的一般形式，通过添加特征$x$组成向量$\overrightarrow x$，能够提升到多元线性回归方程
2. 模型训练判断条件：损失函数$Loss$，定义为所有数据的标签与预测值差的平方和，单一数据的损失$L(w,x,b)=(w*x+b-y)^2$，总损失$J(\overrightarrow w,\overrightarrow x,\overrightarrow b)=\frac{1}{2}\sum_{i=1}^n\frac{1}{n}(\overrightarrow w*\overrightarrow x+b-\hat y)^2 $，损失最小时模型达到最佳训练效果
3. 最优化：梯度下降$Gradient$，通过损失函数对参数的偏导进行迭代方向的计算$\frac{\partial J}{\partial w}$，通过学习率决定损失函数下降速度，通过adam学习率优化器可以做到动态调整

### 多元回归方程

1. 基础含义：数据满足某种曲线的关系，或者直线关系特征不明显但是拟合某个曲线，通过高阶多项式进行拟合，一般表达式$y=w_1x_1^2+w_2x_2+b$，通过添加高阶多项式能够拟合比较复杂的曲线
2. 模型训练判断条件：损失函数$Loss=\frac{1}{2}\sum_{i=1}^{n}\frac{1}{n}(f(\overrightarrow w*\overrightarrow x)-\hat y)^2$
3. 最优化：通过对所有的参数进行求偏导，使总损失函数最小

### 逻辑方程

1. 定义：包括单元和多元逻辑方程，主要用于分类任务，输出各个标签的预测概率，以$z$代指表达式，与回归方程不同的是最后需要经过$Sigmoid(z)=\frac{1}{1+e^{-z}}$激活函数转换为概率分布，z处于-5和5之间时，函数才能发挥比较好的作用，过大或者过小会导致最终结果变为0或者1
2. 损失函数：$Loss=-\hat ylog(f(w*x+b))-(1-\hat y)log(1-f(w*x+b))$
3. 最优化同理，进行损失函数对参数的偏导

### 易出现的问题

1. 过拟合：模型曲线过于复杂，常见在多元方程中，使得损失函数即使很小，但是表现仍然很差，主要解决方法：
   1. 添加更多数据
   2. 减少特征的使用
   3. 正则化($Dropout$)通过添加一项使参数逐渐缩小，既可以避免直接删除特征，又可以防止特征影响过大
2. 高偏差$High\ bias$：模型不能拟合训练集，主要通过特征工程，减小正则化参数，增加特征等方式
3. 高方差$High\ variance$：模型泛化能力较差，主要通过增大正则化参数，减少特征，增加数据等方式

## 机器学习第二部分

### 神经网络

1. 神经元$Neuron$：拟合人脑神经元效果，每个神经元是一个函数，或是线性方程，或是逻辑方程，多个神经元组合成一个层$Layer$
2. 隐藏层：输入的特征经过神经元组合形成的新的特征，这些神经元所在的层就叫做隐藏层，隐藏层可以是单层也可以是多个层组合，每个神经元有各自的参数，隐藏层接收前一层所有输入，通过设置权重来决定某个神经元使用哪些特征，每个神经元都需要sigmoid进行激活，输入的特征在隐藏层中变换的过程称为前向传播
3. 输出层：根据任务要求，转为概率分布或者对应的标签，如果是单一类别，通过$Sigmoid$激活函数，如果是多类别，通过$Softmax$转换为每个类别的概率分布
4. 卷积层：通过处理输入数据，使其接收到数据的一部分，能够避免过拟合，也能够加快模型速度

### 决策树
